{
  "metadata": {
    "name": "project-capstone-1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, avg, sum, countDistinct, desc, col,lit\n\nspark \u003d SparkSession.builder.appName(\"e-commerce\").enableHiveSupport().getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nlogistics_df \u003d spark.read.csv(\"hdfs://namenode:9000/user/menna/logistics_info.csv\",\r\n                              header\u003dTrue, inferSchema\u003dTrue)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\nfrom pyspark.sql import DataFrame\r\n\r\n# Drop the column\r\nlogistics_df \u003d logistics_df.drop(\" actual_delivery_date\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlogistics_df.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\norder_df \u003d spark.read.csv(\"hdfs://namenode:9000/user/menna/order_info.csv\",\r\n                              header\u003dTrue, inferSchema\u003dTrue)\r\norder_df.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\norder_df \u003d order_df.drop(\"_c6\",\"_c7\", \"_c8\", \"_c9\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\norder_df.show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.ml.feature import StringIndexer\n\n# Create StringIndexer for country to make each country has its own warehouse id\nindexer \u003d StringIndexer(inputCol\u003d\"order_country\", outputCol\u003d\"warehouse_id\")\n\nindexer_model \u003d indexer.fit(order_df)\n\norder_df \u003d indexer_model.transform(order_df)\n\norder_df.select(\"order_id\", \"order_country\", \"warehouse_id\").show(10, truncate\u003dFalse)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\nnum_countries \u003d order_df.select(\"order_country\").distinct().count()\nprint(num_countries)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ncountry_counts \u003d order_df.groupBy(\"order_country\").count()\ncountry_counts.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import functions as F\n\norder_df \u003d order_df.withColumn(\"expected_delivery_date\", F.date_add(F.col(\"order_date\"), 5))\norder_df.show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogistics_df \u003d logistics_df.join(\n    order_df.select(\"order_id\", \"expected_delivery_date\"),\n    on\u003d\"order_id\",\n    how\u003d\"left\"\n)\n\nlogistics_df \u003d logistics_df.withColumn(\n    \"actual_delivery_date\",\n    F.when(F.rand() \u003c 0.33, F.date_sub(F.col(\"expected_delivery_date\"), 1))  # early\n     .when(F.rand() \u003c 0.66, F.col(\"expected_delivery_date\"))                # on time\n     .otherwise(F.date_add(F.col(\"expected_delivery_date\"), 2))             # late\n)\nlogistics_df \u003d logistics_df.drop(\"expected_delivery_date\")\nlogistics_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nlogistics_df.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\nfrom pyspark.sql import functions as F\r\n\r\n# Make sure warehouse_id and shipping_cost are included from logistics_df\r\njoined_df \u003d logistics_df.join(\r\n    order_df.select(\"order_id\", \"expected_delivery_date\",\"warehouse_id\"),\r\n    on\u003d\"order_id\",\r\n    how\u003d\"left\"\r\n)\r\n\r\n# Calculate delivery delta and late delivery\r\njoined_df \u003d joined_df.withColumn(\r\n    \"delivery_time_delta_days\",\r\n    F.datediff(F.col(\"actual_delivery_date\"), F.col(\"expected_delivery_date\"))\r\n).withColumn(\r\n    \"is_late_delivery\",\r\n    F.when(F.col(\"delivery_time_delta_days\") \u003e 0, True).otherwise(False)\r\n)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Select the columns that exist in joined_df\nStaging_Logistics_Fact \u003d joined_df.select(\n    \"order_id\",\n    \"delivery_time_delta_days\",\n    \"is_late_delivery\",\n    \"shipping_cost\",\n    \"warehouse_id\"\n)\n\nStaging_Logistics_Fact.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import round, col\n\n# Round order_id and shipping_cost to 0 decimal places (nearest integer)\nStaging_Logistics_Fact \u003d Staging_Logistics_Fact.withColumn(\"order_id\", round(col(\"order_id\"), 0)) \\\n               .withColumn(\"shipping_cost\", round(col(\"shipping_cost\"), 0))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nStaging_Logistics_Fact.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nStaging_Logistics_Fact.groupBy(\"is_late_delivery\").count().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(Staging_Logistics_Fact.columns)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nStaging_Logistics_Fact.createOrReplaceTempView(\"staging_logistics_fact\")\n\navgDeliveryTimeDelta \u003d spark.sql(\"\"\"\n    SELECT \n        warehouse_id,\n        AVG(delivery_time_delta_days) AS avg_delivery_time_delta\n    FROM staging_logistics_fact\n    GROUP BY warehouse_id\n\"\"\")\navgDeliveryTimeDelta.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\navgDeliveryTimeDelta.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/user/bigdata/avgDeliveryTimeDelta\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import col\n\nStaging_Logistics_Fact_fixed \u003d Staging_Logistics_Fact.select(\n    col(\"order_id\").cast(\"int\"),\n    col(\"delivery_time_delta_days\").cast(\"int\"),\n    col(\"is_late_delivery\").cast(\"boolean\"),\n    col(\"shipping_cost\").cast(\"float\"),\n    col(\"warehouse_id\").cast(\"int\")\n)\n\nStaging_Logistics_Fact_fixed.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/user/bigdata/staging_logistics_fact\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}